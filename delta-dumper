#! /usr/bin/perl

use strict;
use warnings;
use v5.16;
#use Fatal qw( :void open close link unlink symlink rename fork );
#use autodie; # does NOT include system
# this should cause dd to die when sent kill, ctrl-c etc.
use sigtrap qw(handler signal_handler normal-signals);

use Backup::rdbduprunner qw($APP_NAME $USER $STATE_DIR $LOCK_DIR $LOG_DIR $CONFIG_DIR $DISPATCHER &stringy %databasetype_case &unlock_and_close $timestamp_format $iso8601_regex &dlog &lock_db &lock_pid_file &lock_file_compose $uuid &uuid_gen &callback_format &callback_format_terminal &backup_file_name_builder &unlink_alternates);

# might be assumed to be installed on any perl system by default?
use Data::Dumper;
use Readonly;
use English qw( -no_match_vars );
use File::Basename;
use POSIX;
use Fcntl qw(:DEFAULT :flock); # import LOCK_* constants
use File::Temp qw/ tempfile /;
use Env qw( HOME DEBUG );
use Pod::Usage;
eval { use Time::HiRes qw( time ); };
use File::Spec;
use File::Path qw( make_path );
use Date::Parse;
use JSON;
use GDBM_File;
use Storable qw( freeze thaw );
use IO::Interactive;

# added from CPAN or system packages
use Log::Dispatch;
use Log::Dispatch::Syslog;
use Log::Dispatch::Screen;
use Log::Dispatch::File;
use AppConfig qw(:expand :argcount);
use Getopt::Long qw(:config pass_through) ; # use this to pull out the config file, then let AppConfig handle the rest

##### GLOBALS ##### # the horrror:

# list of readonly periods:
Readonly my @periods => qw( daily weekly monthly );

# list of readonly known database types:
Readonly my @dbtypes => qw( mysql postgresql mongodb );

# global app configuration
my $config;

my $DB_FILE = File::Spec->catfile($STATE_DIR, "${APP_NAME}.db");
my $CHECKSUM_DB = File::Spec->catfile($STATE_DIR, "${APP_NAME}-checksum.db");

# create our state, config, lock and log directories if they don't exist:
Backup::rdbduprunner::make_dirs();

# the path to the config file to be parsed
my $CONFIG_FILE = File::Spec->catfile($CONFIG_DIR, 'config');

# much loved, storied, and magical restore mode!
my $RESTORE;
# if true, print the database status and exit
my $STATUS_JSON;
# delete these keys from the status database
my @STATUS_DELETE;
# dump the checksums and exit
my $CHECKSUM_DUMP;
# validate and prune the database:
my $CHECKSUM_FSCK;
# verify the checksums and exit
my $CHECKSUM_VERIFY_ALL;
# verify just one dump:
my $CHECKSUM_VERIFY;

# create a logparams for logging, intended to be localized:
our %logparams=();

# data structure for finding the most recent dumps and protecting used
# ones:
my %dump_db;

# store a list of temp files which dd will attempt to remove if it
# catches a normal signal:
my @temp_files;

######## MAIN ########

main();

######## SUBROUTINES ########
# delete all the temporary files, ignoring errors and die
sub signal_handler {
    my ($sig) = @_;
    eval {
        foreach my $f (@temp_files) {
            next unless -f $f;
            unlink $f;
            warn "unlink ${f}";
        }
    };
    die "caught SIG${sig}, exiting after cleanup";
}

# create the dispatcher and return it
sub create_dispatcher {
    my $disp=Log::Dispatch->new();

  if ($config->sys_logging) {
    $disp->add(Log::Dispatch::Syslog->new(name      => 'syslog',
                                          min_level => $config->level,
                                          ident     => $APP_NAME.'['.$$.']',
                                          facility  => $config->facility,
                                          socket    => 'unix',
                                          callbacks => \&callback_format
                                      )
              );
  }
  if ($config->terminal_logging) {
    $disp->add(Log::Dispatch::Screen->new(name      => 'screen',
                                          min_level => $config->log_level,
                                          stderr    => 1,
                                          newline   => 1,
                                          callbacks => IO::Interactive::is_interactive() ? \&callback_format_terminal : \&callback_format,
                                         )
              );
  }
  if ($config->file_logging) {
    $disp->add(
               Log::Dispatch::File->new(
                                        name      => 'logfile',
                                        min_level => $config->level,
                                        filename  => File::Spec->catfile($LOG_DIR,"${APP_NAME}.log-".strftime('%Y%m%d',localtime(time()))),
                                        mode      => '>>',
                                        newline   => 1,
                                        callbacks => \&callback_format
          )
              );
  }
  return $disp;
}

sub build_mysql_exec {
  my $bin = shift;
  my @mysql_options = ($config->mysql_bindir ? File::Spec->catfile($config->mysql_bindir,$bin) : $bin);
  if ($config->mysql_defaults_file) {
    push(@mysql_options, '--defaults-file='.$config->mysql_defaults_file);
  }
  if($bin eq 'mysqldump') {
    if ($config->mysql_ignore_table and @{$config->mysql_ignore_table} > 0) {
      foreach my $t (@{$config->mysql_ignore_table}) {
        push(@mysql_options,"--ignore-table=${t}");
      }
    }
    if ($config->mysql_single_transaction) {
      push(@mysql_options, '--single-transaction');
    }
    if ($config->mysql_extra_option and scalar @{$config->mysql_extra_option} > 0) {
        push(@mysql_options, @{$config->mysql_extra_option});
    }
}
  my %mopts = (
      '-u' => 'user',
      '-h' => 'hostname',
      '-p' => 'password',
  );
  while(my ($p,$c) = each(%mopts)) {
      if ( $config->get('mysql_'.$c) ) {
          push(@mysql_options, $p.$config->get('mysql_'.$c));
      }
  }
  return @mysql_options;
}

sub build_mongodb_exec {
    my @options
        = ( $config->mysql_bindir
        ? File::Spec->catfile( $config->mysql_bindir, 'mongodump' )
        : 'mongodump' );
    push @options, '--archive';
    foreach my $p (qw( username hostname password port )) {
        if ( $config->get('mongodb_'.$p) ) {
            if ( $p eq 'hostname' ) {
                # hostname is --host but for laziness sake I'll fix it here:
                push(@options, '--host='.$config->get('mongodb_'.$p));
            }
            else {
                push(@options, '--'.$p.'='.$config->get('mongodb_'.$p));
            }
        }
    }
    if ( $config->mongodb_ssl ) {
        push( @options, '--ssl' );
    }
    return @options;
}

sub mysql_database_list {
  my $mysql_com = join(' ',build_mysql_exec('mysql'));
  my $mysql_handle;
  my @dl;
  if (open $mysql_handle, '-|', "${mysql_com} -e \"SHOW DATABASES;\"") {
    my $c=0;
    foreach (<$mysql_handle>) {
      chomp $_;
      push(@dl,$_) unless $c == 0;
      $c++;
    }
  }
  else {
    dlog('critical','unable to launch mysql for listing databases',{return_code => POSIX::WEXITSTATUS($?)});
  }
  return @dl;
}

sub pgsql_database_list {
    my $psql_com = join(' ',build_postgresql_exec('psql'));
    my @dl;
    my $psql_handle;
    if (open $psql_handle, '-|', "su -c \"${psql_com} -q -t -c \\\"SELECT datname FROM pg_catalog.pg_database WHERE datistemplate = false;\\\"\" " . $config->postgresql_username) {
        foreach (<$psql_handle>) {
            chomp $_;
            if(/\s+(.+)$/) {
                push(@dl,$1);
            }
        }
    }
    else {
        dlog('critical','unable to launch psql for listing databases',{return_code => POSIX::WEXITSTATUS($?)});
    }
    return @dl;
}

sub init_config {
    $config = AppConfig->new(
        { GLOBAL => { EXPAND => EXPAND_VAR | EXPAND_ENV, }, } );

    $config->define(
        'dry_run' => {
            ALIAS    => 'dry-run',
            ARGCOUNT => ARGCOUNT_NONE,
            ARGS     => '!',
        },
        'log_level' => {
            ALIAS    => "loglevel|level|log-level",
            DEFAULT  => 'info',
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
            VALIDATE =>
                '^(debug|info|notice|warning|error|critical|alert|emergency)$'
        },
        'facility' => {
            DEFAULT  => 'user',
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
            VALIDATE =>
                '^(auth|authpriv|cron|daemon|kern|local[0-7]|mail|news|syslog|user|uucp)$',
        },
        'checksum' => {
            ARGS     => '!',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'checksum_algorithm' => {
            ARGS     => '=s',
            ALIAS    => 'checksum-algorithm',
            DEFAULT  => 'sha256',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'checksum_binaries' => {
            ARGS    => '=s%',
            DEFAULT => {
                'sha256'   => 'sha256sum',
                'md5'      => 'md5sum',
                'sha512'   => 'sha512sum',
            },
            ARGCOUNT => ARGCOUNT_HASH,
        },
        'mbuffer' => {
            ARGS     => '!',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'mbuffer_binary' => {
            ARGS     => '=s',
            ALIAS    => 'mbuffer-binary',
            DEFAULT  => 'mbuffer',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mbuffer_opts' => {
            ARGS     => '=s',
            DEFAULT  => '-v 1',
            ALIAS    => 'mbuffer-opts',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'rate_limit' => {
            ARGS     => '=s',
            ALIAS    => 'rate-limit',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_skip_database' => {
            DEFAULT =>
                [ '#lost+found', 'performance_schema', 'information_schema' ],
            ARGCOUNT => ARGCOUNT_LIST,
            ARGS     => '=s@',
        },
        'postgresql_skip_database' => {
            DEFAULT  => ['#lost+found'],
            ARGCOUNT => ARGCOUNT_LIST,
            ARGS     => '=s@',
        },
        'compression' => {
            DEFAULT  => 'gzip',
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
        },
        'secondary_compression' => {
            DEFAULT  => 'gzip',
            ARGCOUNT => ARGCOUNT_ONE,
            ARGS     => '=s',
        },
        'compressor_compress' => {
            ARGS    => '=s%',
            ALIAS   => 'compressor-compress',
            DEFAULT => {
                'gzip'     => 'gzip -c',
                'bzip2'    => 'bzip2 -c',
                'xz'       => 'xz --stdout',
                'zstd'     => 'zstd --stdout --rsyncable',
                'compress' => 'compress -c',
                'none'     => 'cat',
                'xdelta'   => 'xdelta3 -e -D',
                'lz4'      => 'lz4 --stdout',
            },
            ARGCOUNT => ARGCOUNT_HASH,
        },
        'compressor_uncompress|compressor-uncompress=s%' => {
            DEFAULT => {
                'gzip'     => 'gzip -d -c',
                'bzip2'    => 'bzcat',
                'xz'       => 'xzcat',
                'zstd'     => 'zstd --uncompress --stdout',
                'compress' => 'uncompress -c',
                'none'     => 'cat',
                'xdelta'   => 'xdelta3 -d -D',
                'lz4'      => 'lz4 --uncompress --stdout',
            },
        },
        'compressor_suffix|compressor-suffix=s%' => {
            DEFAULT => {
                'gzip' => 'gz',
                'bzip2' => 'bz2',
                'xz' => 'xz',
                'zstd' => 'zst',
                'compress' => 'Z',
                # 'none' => 'cat', # leave undefined
                'xdelta' => 'vcdiff',
                'lz4' => 'lz4',
            },
        },
        'backup_location=s' => {
            DEFAULT => "/var/${APP_NAME}/backups",
            VALIDATE => '^\/.+',    # require an absolute path?
        },
        'old_backup_location' => {
            ALIAS    => 'old-backup-location',
            ARGS     => '=s@',
            ARGCOUNT => ARGCOUNT_LIST,
        },
        'tmpdir' => {
            ALIAS    => 'tmp_dir|tempdir|temp_dir',
            ARGS     => '=s',
            VALIDATE => '^\/.+',       # require an absolute path?
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'dump_config' => {
            ALIAS    => 'dump-config',
            ARGS     => '!',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'h|help!',
        'mysql_bindir' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\/.+',        # require an absolute path?
        },
        'postgresql_bindir' => {
            DEFAULT  => '/usr/bin',
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\/.+',        # require an absolute path?
        },
        'postgresql_extra_option' => {
            DEFAULT  => ['--clean','--if-exists'],
            ARGS     => '=s@',
            ARGCOUNT => ARGCOUNT_LIST,
        },
        'postgresql_username' => {
            DEFAULT  => 'postgres',
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'postgresql_host' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql!',
        'postgresql!',
        'mongodb!',
        'rsyncable!',
        'rsyncable_location' => {
            ALIAS    => 'rsyncable-location',
            ARGS     => '=s',
            VALIDATE => '^\/.+',       # require an absolute path?
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'rsyncable_compression' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            DEFAULT  => 'zstd',
        },
        'sys_logging' => {
            DEFAULT  => 1,
            ARGS     => '!',
            ALIAS    => 'syslog',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'sys_logging' => {
            DEFAULT  => 1,
            ARGS     => '!',
            ALIAS    => 'syslog',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'terminal_logging' => {
            # if stderr is a terminal, enable terminal_loging
            DEFAULT  => IO::Interactive::is_interactive() ? 1 : 0,
            ARGS     => '!',
            ALIAS    => 't',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'file_logging' => {
            DEFAULT  => 1,
            ARGS     => '!',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'mysql_defaults_file' => {
            ALIAS    => 'mysql_defaults-file|defaults-file',
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_single_transaction' => {
            DEFAULT  => 1,
            ARGS     => '!',
            ALIAS    => 'single-transaction|mysql_single-transaction',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'mysql_ignore_table' => {
            DEFAULT  => ['mysql.events'],
            ARGS     => '=s@',
            ALIAS    => 'mysql_ignore-table|ignore-table',
            ARGCOUNT => ARGCOUNT_LIST,
        },
        'mysql_user' => {
            ARGS     => '=s',
            ALIAS    => 'mysql-user',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_password' => {
            ARGS     => '=s',
            ALIAS    => 'mysql-password',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_hostname' => {
            ARGS     => '=s',
            ALIAS    => 'mysql-hostname',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mysql_extra_option' => {
            ARGS     => '=s@',
            ARGCOUNT => ARGCOUNT_LIST,
        },
        'mysql_verify' => {
            ARGS     => '!',
            ALIAS    => 'mysql-verify',
            ARGCOUNT => ARGCOUNT_NONE,
        },
        'daily' => {
            ARGS     => '=i',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\d+$',
        },
        'weekly' => {
            ARGS     => '=i',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\d+$',
        },
        'monthly' => {
            ARGS     => '=i',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\d+$',
        },
        'week_start' => {
            DEFAULT  => 'Sun',
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^(Sun|Mon|Tue|Wed|Thu|Fri|Sat)$',
        },
        'month_start' => {
            DEFAULT  => '1',
            ARGS     => '=i',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^[0-9]+$',
        },
        'maxage' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^[0-9]+[hdwmy]$',
        },
        'rsync_binary' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            DEFAULT  => 'rsync',
        },
        'rsync_options' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            DEFAULT  => '-a --inplace --no-whole-file',
        },
        'prerun' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'postrun' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_bindir' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
            VALIDATE => '^\/.+',        # require an absolute path?
        },
        'mongodb_username' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_password' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_port' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_hostname' => {
            ARGS     => '=s',
            ARGCOUNT => ARGCOUNT_ONE,
        },
        'mongodb_ssl' => {
            DEFAULT  => 1,
            ARGS     => '!',
            ARGCOUNT => ARGCOUNT_NONE,
        },
    );
}

sub main {
    chdir('/') or die 'unable to chdir to /';
  init_config();
  parse_config();

  # set up our dispatch destinations:
  $DISPATCHER = create_dispatcher();

  # major modes of operation follow:
  if ( @STATUS_DELETE and scalar @STATUS_DELETE > 0 ) {
      status_delete(@STATUS_DELETE);
      exit;
  }
  if ( basename($PROGRAM_NAME) eq 'check_delta_dumper' or $STATUS_JSON ) {
      status_json();
      exit;
  }
  if ( $CHECKSUM_DUMP ) {
      checksum_dump();
      exit;
  }
  if ( $CHECKSUM_FSCK ) {
      checksum_fsck();
      exit;
  }
  if ( $CHECKSUM_VERIFY_ALL ) {
      checksum_verify_all();
      exit;
  }
  if ( $CHECKSUM_VERIFY ) {
      checksum_verify($CHECKSUM_VERIFY);
      exit;
  }

  if ( $RESTORE ) {
      restore($RESTORE);
      exit;
  }

  ## Starting up:
  my $RUNTIME=time();
  dlog('info','starting',{});

  ## Dump the config if request, and exit:
  if ($config->dump_config) {
      print Dumper $config;
    die "configuration dump requested and delivered, exiting";
  }

  sanity_check_config();

  %dump_db = %{scan_existing()};

  # obtain the global lock, or die trying
  my $GLOBAL_LOCK = $config->dry_run ? 1 : lock_pid_file(30,$APP_NAME);
  unless ( $GLOBAL_LOCK ) {
    my $emsg='unable to lock pid file: '.lock_file_compose($APP_NAME);
    dlog('warning',$emsg,{});
    die $emsg;
  }

  unless ( -d $config->backup_location ) {
    dlog('critical', 'backup location does not exist', { backup_location => $config->backup_location });
      die "output directory does not exist and WILL NOT be created: ".$config->backup_location;
  }
  # create sub-directories:
  foreach my $p (@periods,'current') {
    my $ddir = File::Spec->catfile($config->backup_location,$p);
    unless ( $config->dry_run or -d $ddir or mkdir($ddir)) {
      dlog('critical',
           "output directory does not exist and cannot be created: ${ddir}",
           { backup_location => $config->backup_location, ddir => $ddir });
      die "output directory does not exist and cannot be created: ${ddir}";
    }
  }
  if ( $config->prerun ) {
      unless( system_runner($config->prerun) ) {
          unlock_and_close($GLOBAL_LOCK);
          dlog('error',
               'exiting because prerun command failed',
               {'total_run_time_seconds' => time()-$RUNTIME});
          exit;
      }
  }
 DBTYPE:
  foreach my $dbtype (qw(mysql postgresql mongodb)) {
      next unless $config->get($dbtype);
  DB:
      foreach my $db ( $dbtype eq 'mysql' ? mysql_database_list()
                       : ( $dbtype eq 'postgresql' ? pgsql_database_list()
                       : ( $dbtype eq 'mongodb' ? qw(all)
                       : ())) ) {
          $uuid = uuid_gen();
          if (( $dbtype eq 'mysql'
               and $config->mysql_skip_database
                and string_any($db,@{$config->mysql_skip_database}) )
              or
              ( $dbtype eq 'postgresql'
                and $config->postgresql_skip_database
                and string_any($db,@{$config->postgresql_skip_database}) ))
          {
              dlog('notice',
                   "skipping database ${dbtype}:${db}",
                   { databasetype => $dbtype, databasename => $db });
              next DB;
          }
          eval {
              perform_backup($dbtype,$db);
          } or do {
              my $e = $@;
              dlog('error',
                   "exception performing dump of ${dbtype}:${db} ${e}",
                   { 'databasetype' => $dbtype, 'databasename' => $db, 'exception' => $e });
              die $e if $e =~ m{caught \s+ SIG .+ exiting}xms;
              next DB;
          }
      }
  }

  if ( $config->postrun ) {
      system_runner($config->postrun);
  }

  unlock_and_close($GLOBAL_LOCK) unless $config->dry_run;
  dlog('info',
       'exiting',
       {'total_run_time_seconds' => time()-$RUNTIME});
}

# returns 1 for success or null for failure?
# needs work
sub system_runner {
    my %logparams;
    local %ENV = %ENV;
    my $prefix = uc $APP_NAME;
    $prefix =~ s/\-/_/g;

    my $cmd = shift;
    $logparams{cmd} = $cmd;

    my %pairs = ('BACKUP_LOCATION' => $config->backup_location,
                 'LOG_DIR'         => $LOG_DIR,
                 'TMPDIR'          => $config->tmpdir,
                 'COMPRESSION'     => $config->compression,
                 'COMPRESS'        => $config->compressor_compress->{$config->compression},
                 'UNCOMPRESS'      => $config->compressor_uncompress->{$config->compression},
                 'SUFFIX'          => $config->compressor_suffix->{$config->compression},
             );
    while ( my ($k,$v) = each %pairs) {
        $ENV{join('_',$prefix, uc $k)} = $v;
    }
    dlog('debug',
         "system('${cmd}')",
         \%logparams);
    unless ($config->dry_run) {
        unless ( POSIX::WIFEXITED(system( $cmd ) ) ) {
            $logparams{errno} = "${ERRNO}";
            dlog('error',
                 "failed with errno: $logparams{errno} system('${cmd}')",
                 \%logparams );
            return;
        }
        $logparams{exit} = POSIX::WEXITSTATUS($CHILD_ERROR);
        unless ( $logparams{exit} == 0 ) {
            dlog('error',
                 "failed with WEXITSTATUS: $logparams{exit} system('${cmd}')",
                 \%logparams );
            return;
        }
    }
    dlog('info',
         "succeeded: system('${cmd}')",
         \%logparams );
    return 1;
}

sub parse_config {
  ## Configuration file parsing:
  # parse --config command line options, removing it from @ARGV
  GetOptions('config_file|config|config-file=s'         => \$CONFIG_FILE,
             'status_json|status-json!'                 => \$STATUS_JSON,
             'status_delete|status-delete=s@'           => \@STATUS_DELETE,
             'checksum_dump|checksum-dump!'             => \$CHECKSUM_DUMP,
             'checksum_fsck|checksum-fsck!'             => \$CHECKSUM_FSCK,
             'checksum_verify|checksum-verify=s'        => \$CHECKSUM_VERIFY,
             'checksum_verify_all|checksum-verify-all!' => \$CHECKSUM_VERIFY_ALL,
             'restore=s'                                => \$RESTORE,
            );
  # read the config file
  if ( -f $CONFIG_FILE ) {
    $config->file($CONFIG_FILE);
  } else {
    warn "configuration file does not exist (${CONFIG_FILE}), proceeding from command line options only";
  }
  ## Command Line Parsing:
  $config->getopt;
  # if there is anything left, we must acquit:
  if (@ARGV > 0) {
    die "unparsed remaining command line options: @{ARGV}";
  }

  # print usage if request:
  pod2usage(-1) if $config->help;
}

# called perform backup, however it is mostly concerned with what
# happens after the backup/dump is performed
sub perform_backup {
    my $databasetype = shift;
    my $database = shift;
    local %logparams = (
        databasetype    => $databasetype_case{$databasetype},
        compression     => $config->compression,
        backup_location => $config->backup_location,
        databasename    => $database,
    );
    # record the time for logging:
    my $start_time = time();
    # suffix or not:
    my @suffix = ( 'sql' ) ;
    # this is a hash from the dump_db with keys, timestamp, path, full, and period
    my $full_dump = find_most_recent_dump($databasetype,$database);

    my $effective_compression_method
        = $config->compression eq 'xdelta'
        ? ($full_dump
           ? 'xdelta' # ask and ye shall receive
           : $config->secondary_compression)
        : $config->compression;
    if ( defined $config->compressor_suffix->{$effective_compression_method} ) {
        push @suffix, $config->compressor_suffix->{$effective_compression_method};
    }
    $logparams{compression} = $effective_compression_method;

    # the components of the file name to eventually create:
    my @file_parts = ( $databasetype, $database );

    # this timestamp should be used in the creation of the symlinks:
    my $timestamp_format_start_time = strftime $timestamp_format, localtime($start_time);
    # this is the name of the file/link that will be placed in the
    # daily/weekly/monthly directory after a successful backup:
    my $file_name_timestamp = backup_file_name_builder(
        @file_parts,
        $effective_compression_method eq 'xdelta'
        # two timestamps, the full dump and the new:
        ? join(
            '_',
            strftime(
                $timestamp_format, localtime( $$full_dump{timestamp} )
            ),
            $timestamp_format_start_time,
            )
        # only one timestamp:
        : $timestamp_format_start_time,
        \@suffix
    );
    # output_path is a full path to a temporary file to initially dump
    # the database into:
    my ($_of,$output_path,$_rf,$rsyncable_output_path);
    if ( $config->dry_run ) {
        $output_path
            = File::Spec->catfile(
            $config->tmpdir ? $config->tmpdir : $config->backup_location,
            "${file_name_timestamp}.XXXXXXXX" );
        $rsyncable_output_path
            = File::Spec->catfile(
            $config->tmpdir ? $config->tmpdir : $config->backup_location,
            "${file_name_timestamp}.rsync.XXXXXXXX" );
    }
    else {
        ( $_of, $output_path ) = tempfile( "${file_name_timestamp}.XXXXXXXX",
            DIR => $config->tmpdir
            ? $config->tmpdir
            : $config->backup_location )
            or die "failed to create temp file";
        push @temp_files, $output_path;
        if($config->rsyncable and $effective_compression_method ne $config->rsyncable_compression) {
            ( $_rf, $rsyncable_output_path ) = tempfile( "${file_name_timestamp}.rsync.XXXXXXXX",
                                                         DIR => $config->tmpdir
                                                         ? $config->tmpdir
                                                         : $config->backup_location )
                or die "failed to create temp file";
            push @temp_files, $rsyncable_output_path;
        }
        else {
            #$_rf = $_of;
            $rsyncable_output_path = $output_path;
        }
    }
    $logparams{output_path} = $output_path;
    $logparams{start_time} = $timestamp_format_start_time;
    $logparams{rsyncable_output_path} = $rsyncable_output_path if $config->rsyncable;

    # this is historical, clean up the junk!
    link_symlinks( $databasetype, $database );

    dlog('debug', "perform_backup is calling generic_dump on ${databasetype}:${database} using ${effective_compression_method} to ${output_path}",
         {
             'msgcode' => 'perform_backup',
         },
         \%logparams);
    my ($return_value,$stage)
        = generic_dump($databasetype,
                       $database,
                       $start_time,
                       $effective_compression_method,
                       $output_path,
                       $rsyncable_output_path,
                       $full_dump,
                   );

    return $start_time if $config->dry_run;

    unless (defined $return_value and $return_value == 0) {
        # presumed to have failed... I think we need to clean up manually
        dlog('info','perform cleanup', \%logparams);
        unlink $output_path if -f $output_path;
        update_status_db($databasetype, $database, { exit => $return_value,
                                                     stage => $stage,
                                                     start_time => $start_time });
        dlog('error',"failed: backup of ${databasetype}:${database} exited ${return_value}",
             \%logparams,
             { 'msgcode' => 'perform backup fall through failure' });
        return;
    }

    if ($config->rsyncable) {
        # this is the directory we intend to write the dump to:
        my $rsync_dir = $config->rsyncable_location ?
            $config->rsyncable_location :
            File::Spec->catfile($config->backup_location,'current');
        my @suffers = ( 'sql' );
        unless($config->rsyncable_compression eq 'none') {
            push(@suffers,$config->compressor_suffix->{$config->rsyncable_compression})
        };
        my $rsyncable_file_name =
            backup_file_name_builder(@file_parts,\@suffers);
        my $rsyncable_path = File::Spec->catfile(
            $rsync_dir,
            $rsyncable_file_name);

        my $sum_file;
        if($config->checksum) {
            my $sum;
            $sum_file = File::Spec->catfile(
                $rsync_dir,
                backup_file_name_builder(@file_parts,
                                         ['sql',
                                          $config->checksum_algorithm]));
            if(open($sum, '>', $sum_file)) {
                my $_checksum = checksum_lookup($databasetype,$database,$start_time);
                my ($algo,$checksum) = checksum_lookup($databasetype,$database,$start_time);
                # literally two spaces, look it up!
                print $sum sprintf('%s  %s'."\n",
                                   $checksum,
                                   backup_file_name_builder(@file_parts,
                                                            ['sql']));
                close $sum;
            }
            else {
                dlog('error',
                     "unable to write checksum file ${sum_file}",
                     {msgcode => 'checksum file',
                      sum_file => $sum_file});
            }
        }
        # create a useful list of alternate possible last backups:
        my @alternates = path_alternates($rsyncable_path);
        # look for postgres-* in the current directory if we are backing up postgresql
        if ( $databasetype eq 'postgresql' ) {
            push(
                @alternates,
                path_alternates(
                    File::Spec->catfile(
                        $rsync_dir,
                        backup_file_name_builder(
                            'postgres', $database, \@suffix
                        )
                    )
                )
            );
        }
        print Dumper \@alternates if $DEBUG;
        if ( rsync($rsyncable_output_path, $rsyncable_path) ) {
            unlink_alternates([$rsyncable_path, $sum_file], @alternates);
        } else {
            dlog('error',
                 "unable to rsync from output path (${rsyncable_output_path}) to backup path (${rsyncable_path})", \%logparams);
        }
    }

    # initially attempt to hard link the output_path to the daily file
    my $link_target = $output_path;
 PERIOD:
    foreach my $p (qw(daily weekly monthly)) {
        # do the weekly if needed:
        if ( $p eq 'weekly' and strftime( "%a", localtime($start_time) ) ne $config->week_start ) {
            next PERIOD;
        }
        if ( $p eq 'monthly' and strftime( "%d", localtime($start_time) ) ne sprintf('%02d', $config->month_start) ) {
            next PERIOD;
        }
        my $period_path =
            File::Spec->catfile( $config->backup_location,
                                 $p,
                                 $file_name_timestamp);
        if (link $link_target, $period_path) {
            $link_target = $period_path;
            dlog('debug', "success link/copy ${p} ${period_path}",
                 {
                     msgcode => 'link/copy',
                     period => $p,
                     path => $period_path,
                 });
        } else {
            my $errno = $ERRNO;
            # just in case:
            unlink $period_path if -f $period_path;
            dlog('debug',
                 "unable to hard link ${period_path} -> ${link_target} with errno: ${errno}",
                 {$p => $period_path,'errno' => $errno},
                 \%logparams);

            my $rsync_result = rsync($link_target, $period_path);
            if ($rsync_result == 1) {
                # success!
                $link_target = $period_path;
            } else {
                unlink $period_path if -f $period_path;
                unlink $output_path if -f $output_path;
                dlog('error',
                     "failed: backup of ${databasetype}:${database}",
                     {
                         msgcode => 'backup status', 'exit' => 1},
                     \%logparams);
                update_status_db(
                    $databasetype,
                    $database,
                    {   exit       => 1,
                        stage      => 'rsync',
                        start_time => $start_time,
                    }
                );
                return;         # failure of sorts
            }
        }
        # successful execution continues here!
        # we update this data structure so when clean_old_dumps runs, it
        # has the most recent successful run in the database:
        insert_into_dump_db(\%dump_db,$databasetype, $database, {
            timestamp      => $start_time,
            path           => $period_path,
            period         => $p,
            full           => $effective_compression_method eq 'xdelta' ? 0 : 1,
            full_timestamp => $effective_compression_method eq 'xdelta' ? $$full_dump{timestamp} : undef,
        });
        print Dumper \%dump_db if $DEBUG;
    }
    unlink $output_path if -f $output_path;
    unlink $rsyncable_output_path if -f $rsyncable_output_path;
    dlog('notice',
         "succeeded: backup of ${databasetype}:${database}",
         {
             msgcode => 'backup status'},
         \%logparams);
    update_status_db($databasetype, $database, { exit => 0,
                                                 start_time => $start_time, });

    clean_old_dumps($databasetype, $database);
    return $start_time;
}

sub insert_into_dump_db {
    my $dump_db = shift;
    my $dbtype = shift;
    my $db = shift;
    my $dump = shift;
    unless ( defined $$dump_db{$dbtype}{$db} ) {
        $$dump_db{$dbtype}{$db} = [];
    }
    push(@{$$dump_db{$dbtype}{$db}},$dump);
}

sub timestamp_sort {
    return int($$a{timestamp}) <=> int($$b{timestamp});
}
sub find_most_recent_dump {
    local %logparams = %logparams;
    my $databasetype = shift;
    my $databasename = shift;
    unless(exists $dump_db{$databasetype}{$databasename}) {
        return; # fail to find
    }
    my @vals = @{$dump_db{$databasetype}{$databasename}};
    @vals = grep {$$_{full}} @vals;
    return unless scalar @vals > 0;
    @vals = sort timestamp_sort @vals;
    my $dump_stamp = $vals[-1]{ timestamp }; # my favorite musical genre
    my $maxage_seconds = $config->maxage ? parse_time_spec( $config->maxage ) : undef;
    if ( $config->maxage
         and $maxage_seconds
         and $maxage_seconds > 0
         and ( $dump_stamp + $maxage_seconds ) < time() ) {
        dlog('info',
             "backup age",
             \%logparams,
             {   maxage         => $config->maxage,
                 maxage_seconds => $maxage_seconds,
                 dump_timestamp => $dump_stamp,
             }
         );
        return;    # fail to find because of age
    }
    return $vals[-1];
}

sub parse_time_spec {
    my $spec = shift @_;
    my ($t,$p);
    unless( ($t,$p) = $spec =~ m{ \A (\d+) \s* ([hdwmy]) }xms ) {
        dlog('error', "parse_time_spec failure", { spec => $spec });
        return;
    }
    return $t * ( $p eq 'h' ? (3600)
                  : $p eq 'd' ? ( 3600 * 24 )
                  : $p eq 'w' ? ( 3600 * 24 * 7 )
                  : $p eq 'm' ? ( 3600 * 24 * 30 )
                  : ( 3600 * 24 * 365 )
                  );
    # the last option must be y
}

sub scan_existing {
    my $dump_db = {};
    local %logparams = %logparams;
    # pattern for finding delta-dumper files in per-period directories:
    my @locations = ($config->backup_location);
    if ( $config->old_backup_location and ( scalar $config->old_backup_location > 0 ) ) {
        push(@locations, @{$config->old_backup_location});
    }
    my $delta_pat = File::Spec->catfile('{'.join(',',@locations).'}',
                                        '{'.join(',',@periods).'}',
                                        '{'.join(',',@dbtypes,'postgres').'}-*sql*');
    # pattern for finding files created by csg-db-backup.pl, not using
    # per-period directories:
    my $csg_db_backup_pat = File::Spec->catfile('{'.join(',',@locations).'}',
                                                '*{'.join(',',@periods).'}*sql*');
    dlog('debug',"patterns delta_pat: ${delta_pat} csg_db_backup_pat: ${csg_db_backup_pat}",
         {delta_pat => $delta_pat,
          csg_db_backup_pat => $csg_db_backup_pat});

 FILE:
    foreach my $file (glob($delta_pat),glob($csg_db_backup_pat)) {
        if (my $dump = build_dump_hash_from_path($file) ) {
            print STDERR Dumper $dump if $DEBUG;
            insert_into_dump_db($dump_db,$$dump{databasetype},$$dump{databasename},$dump);
        }
        else {
            dlog( 'error', "unable to parse information from file", \%logparams );
            next FILE;
        }
    }
    print Dumper $dump_db if $DEBUG;
    return $dump_db;
}

# use daily/weekly/monthly values to remove files
sub clean_old_dumps {
    my $databasetype = shift;
    my $databasename = shift;
    # we shouldn't calculate this on each call, but here we are:
    my %period_times;
    unless ( exists $dump_db{$databasetype}
             and exists $dump_db{$databasetype}{$databasename} ) {
        return;
    }
 PERIOD:
    foreach my $period (@periods) {

        my $n = $config->get($period) ? $config->get($period) : 0;
        $n = $n * 7 if $period eq 'weekly';
        $n = $n * 31 if $period eq 'monthly';
        $n *= 86400; # convert to seconds
        $period_times{$period} = $n;
    }
 FILE:
    foreach my $fhash (@{$dump_db{$databasetype}{$databasename}}) {
        if ( -l $$fhash{path} and -f $$fhash{path} ) {
            next FILE; # we do not act on non-broken symlinks... but maybe we should?
        }
        next unless ( -f $$fhash{path} or -l $$fhash{path} ) ; # do not act on non-regular files
        next unless unused_dump($$fhash{timestamp}, @{$dump_db{$databasetype}{$databasename}});
        my %logparams = (
            'msgcode'        => 'unlink dump',
            'period'         => $$fhash{period},
            'file'           => $$fhash{path},
            'file_timestamp' => $$fhash{timestamp},
            'file_datetime'  => localtime( $$fhash{timestamp} ),
        );
        if( (time() - $$fhash{timestamp}) > $period_times{$$fhash{period}} ) {
            if( unlink $$fhash{path} ) {
                dlog('info',
                     sprintf('unlink dump file %s for being older than %d %s backups (aka %d seconds)',
                             $$fhash{path},
                             $config->get($$fhash{period}) ? $config->get($$fhash{period}) : 0,
                             $$fhash{period},
                             $period_times{$$fhash{period}}),
                     \%logparams );
            }
            else {
                $logparams{errno} = $ERRNO;
                dlog('error',
                     sprintf('failed to unlink dump file %s: errno %s', $$fhash{path}, $logparams{errno}),
                     \%logparams );
            }
        }
    }
}

# look for the timestamp as referenced by another existing
# backup/dump, and return failure if it is in use, return 1 if it
# isn't in use and therefore "unused" and a potential candidate for
# removal in clean_old_dumps()
sub unused_dump {
    my $timestamp = shift;
    foreach my $fh (@_) {
        print Dumper $fh if $DEBUG;
        if ( defined $$fh{full_timestamp} and $$fh{full_timestamp} eq $timestamp ) {
            return;
        }
    }
    return 1;
}

sub path_alternates {
    my $p = shift;
    state @suffixes;
    unless (@suffixes) {
        @suffixes
            = map { '.' . $_; } ( values(%{$config->compressor_suffix}),
                                  keys(%{$config->checksum_binaries}) );
    }
    # create a list of possible alternative backup paths (if compression changes)
    my @alternates = map {
        my ( $f, $d, $s ) = fileparse( $p, @suffixes );
        File::Spec->catfile( $d, $f . $_ );
    } @suffixes;
    my ( $f, $d, $s ) = fileparse( $p, @suffixes );
    push @alternates, File::Spec->catfile( $d, $f );
    return @alternates;
}


sub pipeline {
    # params are an array of hashrefs, with the following possible keys:
    # input_file or input_handle: used for the start of pipe
    #   if both are undefined then close stdin
    # output_file or output_handle: used the end of the pipe
    #   if both are undefined then close stdin
    # pipes: arrayref of hashrefs of commands to execute while being
    #   joined by anonymous pipes:
    #   stage: named stage (required)
    #   exec:  arrayref of command to execute (required)
    #   user: change to this user first (optional)
    my @pipelines = @_;

    # pipes should look like this:
    # { 'stage' => 'compress', 'exec' => ['bzip2','-c'] },
    # etc.
    my %child_map;               # map pids to stage
    my %exit_map;                #map stages to exit codes
    local %logparams = %logparams;
    my @stages;
    foreach my $pipeline (@pipelines) {
        #print STDERR Dumper $pipeline;
        my $input_handle;
        my $output_handle;
        unless ( exists $$pipeline{pipes} and scalar @{$$pipeline{pipes}} > 0 ){
            die "you must specify valid pipes to pipeline";
        }
        my @pipes = @{$$pipeline{pipes}};
        #print STDERR Dumper \@pipes;
        if (defined $$pipeline{input_file}) {
            unless(open($input_handle, '<', $$pipeline{input_file})) {
                $logparams{file} = $$pipeline{input_file};
                $logparams{errno} = $ERRNO;
                dlog('error','unable to open input file',\%logparams);
                die 'unable to open input file: '.$logparams{file};
            }
        }
        elsif ( defined $$pipeline{input_handle} ) {
            $input_handle = $$pipeline{input_handle};
        }
        if (defined $$pipeline{output_file}) {
            unless(open($output_handle, '>', $$pipeline{output_file})) {
                $logparams{file} = $$pipeline{output_file};
                $logparams{errno} = $ERRNO;
                dlog('error','unable to open output file',\%logparams);
                die 'unable to open output file: '.$logparams{file};
            }
        }
        elsif ( defined $$pipeline{output_handle} ) {
            $output_handle = $$pipeline{output_handle};
        }
    POS:
        for my $pos (0..$#pipes) {
            my ($input,$output);

            if ($pos == $#pipes) { # we are the last iteration!
                # also possible the only iteration?
                ($input,$output) = ( $input_handle, $output_handle );
            }
            else {
                my ($pr);
                unless(pipe($pr,$output)) {
                    $logparams{errno} = $ERRNO;
                    dlog('error','unable to call pipe()',\%logparams);
                    die 'unable to call pipe()';
                }
                # last interations read bits
                $input = $input_handle;
                # save the read end of this
                $input_handle = $pr;
            }
            $child_map{split_fork_exec($input,
                                       $output,
                                       ( defined $pipes[$pos]{'user'} ? $pipes[$pos]{'user'} : undef ),
                                       @{$pipes[$pos]{'exec'}}),
                   } = $pipes[$pos]{'stage'};
        }
    }

    my $MAXWAIT=1;
    do {
        $MAXWAIT = $MAXWAIT * 2;
        while ( (my $pid = waitpid(-1, WNOHANG)) > 0 ) {
            dlog('debug',"waitpid in while loop returned child ${pid}",\%child_map);
            if(exists $child_map{$pid}) {
                $exit_map{$child_map{$pid}} = POSIX::WEXITSTATUS(${^CHILD_ERROR_NATIVE});
                dlog( 'debug',
                      "waitpid returned child ${pid} with exit: ".$exit_map{ $child_map{$pid} },
                      \%exit_map );
                delete $child_map{$pid};
                $MAXWAIT=1;
            }
            else {
                dlog( 'error', "waitpid returned child ${pid} not in map",
                      \%exit_map );
            }
        }
        if (scalar(keys(%child_map)) > 0) {
            # set some signal handlers so that we can awak from pause
            local $SIG{CHLD} = sub { };
            local $SIG{ALRM} = sub { };
            dlog('debug',
                 'waiting on '.(scalar(keys(%child_map))).' sub-processes to exit, pausing',
                 \%logparams);
            my $pause_start=time();
            alarm $MAXWAIT;
            pause;
            alarm 0;
            dlog('debug',
                 "paused for: ".sprintf('%.5f seconds',time()-$pause_start),
                 \%logparams);
        };
    } until( scalar(keys(%child_map)) == 0 );
    return(\%exit_map);
}

sub split_fork_exec {
    my $pr = shift;
    my $pw = shift;
    my $user = shift;

    my $pid = fork();
    # I don't think logging is worth anything at this point:
    die "cannot fork" unless defined $pid;

    if ($pid == 0) {
        # child needs to do some stuff:
        if($user) {
            POSIX::setuid(scalar getpwnam($user));
            if ($! != 0) {
                warn "cannot setuid as specified to ${user}";
                exit(1);
            }
        }
        if ($pr) {
            unless(open(STDIN, '<&', $pr)) {
                warn 'cannot duplicate filehandle into STDIN';
                exit(1);
            }
        }
        else {
            close STDIN;
        }
        if ($pw) {
            #close STDOUT;
            # dup2(0,$pw) or die $!;
            unless(open(STDOUT, '+<&', $pw)) {
                warn 'cannot duplicate filehandle into STDOUT';
                exit(1);
            }
        }

        dlog('debug',"split_fork_exec ".join(' ',@_),{msgcode => 'split_fork_exec', 'exec' => join(' ',@_)});
        exec(@_);
        warn 'unable to exec: '.join(' ',@_);
        exit(1);
    }
    else {
        close $pr if $pr;
        close $pw if $pw;
        return $pid;
    }
}

# given a file, check the suffix against known compression suffix and
# return the "method" corresponding with that suffix, or return undef
# if not found
sub find_compressor {
    my $file = shift;
    #warn ($file) if $DEBUG;
    for my $method (keys(%{$config->compressor_suffix})) {
        my $suffix = $config->compressor_suffix->{$method};
        #warn $method, $suffix if $DEBUG;
        if ( $file =~ m{ [.] $suffix $}xms ){
            return $method;
        }
    }
    return;
}

sub generic_dump {
    # pass the type, database, and path? or also the primary dump exec command?
    my $dbtype = shift;
    my $database = shift;
    my $start_time = shift;
    my $compression = shift;
    my $backup_path = shift;
    my $rsyncable_path = shift;
    my $full_dump = shift; # only used if xdelta is in effect
    my %logparams = (
        'dbtype' => $dbtype,
        'database' => $database,
        'compression' => $compression,
        'backup_path' => $backup_path );

    # if you specify a template, it must include the temporary
    # directory if you don't want it to end up in your current
    # directory:
    # this is the output file for checksums, if used!
    my ($cfh,$cfp);
    if($config->checksum) {
        if ($config->dry_run) {
            $cfp = '/tmp/dd.XXXXXXXX';
        } else {
            unless(($cfh,$cfp) = tempfile()) {
                my $m = 'unable to create a temp file?, that is bad';
                dlog('error',$m,\%logparams);
                die $m;
            }
            push @temp_files, $cfp;
        }
    }
    my @pipelines;
    my @mb = ( $config->mbuffer_binary, '-q' );
    if ($config->mbuffer_opts) {
        push @mb, split(/\s+/,$config->mbuffer_opts);
    }
    my @pipes = ({'stage' => 'dump'});
    if ($dbtype eq 'mysql') {
        $pipes[0]{'exec'} = [(build_mysql_exec('mysqldump'),$database)];
    }
    elsif ($dbtype eq 'postgresql') {
        $pipes[0]{'exec'} = [(build_postgresql_exec('pg_dump'),$database)];
        $pipes[0]{'user'} = 'postgres';
    }
    elsif ($dbtype eq 'mongodb') {
        $pipes[0]{'exec'} = [build_mongodb_exec()];
    }
    else {
        my $dmsg = "unknown database type: ${dbtype}";
        dlog('critical',$dmsg, \%logparams);
        die $dmsg;
    }

    # the full path to various named pipes, written to by tee which
    # will only be created if needed:
    my %fifos = (
        verify => join('.',$backup_path,'fifo'),
        hash => join('.',$backup_path,'hash'),
        delta => join('.',$backup_path,'delta'),
        rsync => join('.',$backup_path,'rsync'),
    );

    # effectively just for debugging, limit the rate of the dump
    # program making it easy to kill a stage of the process to check
    # error handling or just observe the whole process running more
    # slow:
    if ($config->rate_limit) {
        push(@pipes, {
            'stage' => 'pv',
            'exec' => [ 'pv', '-L', $config->rate_limit ],
        });
    }

    # mbuffer might make the whole pipeline faster, or it might not:
    if ($config->mbuffer) {
        push(@pipes,
             {
                 'stage' => 'mbuffer-dump',
                 'exec' => \@mb,
             },
         );
    }

    # we will insert a tee into the chain, to write to various
    # processes that want a copy of the whole stream like
    # checksumming, mysql verify and rsyncable output:
    if (($dbtype eq 'mysql' and $config->mysql_verify) or
        $config->checksum or
        ($config->rsyncable and $config->rsyncable_compression ne $compression)) {
        my @tees = ('tee');
        if ($dbtype eq 'mysql' and $config->mysql_verify) {
            push @tees, $fifos{verify};
        }
        if ($config->checksum) {
            push @tees, $fifos{hash};
        }
        if($config->rsyncable and $config->rsyncable_compression ne $compression) {
            push @tees, $fifos{rsync};
        }
        push(@pipes,
             {
                 'stage' => 'tee',
                 'exec'  => \@tees,
             },
         );
        if ($config->mbuffer) {
            push(@pipes,
                 {
                     'stage' => 'mbuffer-tee',
                     'exec' => \@mb,
                 },
             );
        }
    }
    # store this here and now but put it at the end of the pipelines
    my $decomp_hash;
    # here we need to decide if we should create a diff or not
    if ($compression eq 'xdelta') {
        $logparams{xdelta_source} = $$full_dump{path};
        my @delta_exec = split(/\s+/,$config->compressor_compress->{$compression});
        if (my $c = find_compressor($$full_dump{path}) ) {
            warn "use this compressor/uncompressor: ${c}" if $DEBUG;
            push @delta_exec, '-s', $fifos{delta};
            unless($config->dry_run) {
                mkfifo($fifos{delta}, oct(600)) or die "unable to create fifo for decompressing";
                push @temp_files, $fifos{delta};
            }
            $decomp_hash = {
                input_file => $$full_dump{path},
                output_file => $fifos{delta},
                pipes => [
                    { stage => 'uncompress',
                      exec => [split(/\s+/,$config->compressor_uncompress->{$c})],
                  }],
            };
        }
        else {
            # here we allow xdelta direct access to the file
            push @delta_exec, '-s', $$full_dump{path};
        }
        push(@pipes,{
            'stage' => 'xdelta',
            'exec'  => \@delta_exec,
        });
        if ($config->mbuffer) {
            push(@pipes,
                 {
                     'stage' => 'mbuffer-xdelta',
                     'exec' => \@mb,
                 },
             );
        }
    }
    elsif ( $compression ne 'none' ) {
        push(@pipes,{
            'stage' => 'compress',
            'exec' => [
                split(/\s+/,$config->compressor_compress->{$compression})
            ],
        });
        if ($config->mbuffer) {
            push(@pipes,
                 {
                     'stage' => 'mbuffer-compress',
                     'exec' => \@mb,
                 },
             );
        }
    }
    push(@pipelines,
         { output_file => $backup_path,
           pipes       => \@pipes,
       });
    if ($dbtype eq 'mysql' and $config->mysql_verify) {
        unless($config->dry_run) {
            mkfifo($fifos{verify}, oct(600)) or die "unable to create fifo for verifying mysql dump";
            push @temp_files, $fifos{verify};
        }
        my @vpipes;
        if ($config->mbuffer) {
            push (@vpipes,
                  {
                      'stage' => 'mbuffer-verify',
                      'exec'  => \@mb,
                  },
              );
        }
        push(@vpipes,
             {
                 'stage' => 'verify-tail', 'exec' => [ 'tail', '-n', '1' ] },
             { 'stage' => 'verify',
               'exec'  => [ 'grep', '-q', 'Dump completed' ]
           },
         );
        push(@pipelines,{
            input_file  => $fifos{verify},
            output_file => '/dev/null',
            pipes => \@vpipes,
        });
    }
    if ($config->checksum) {
        unless($config->dry_run) {
            mkfifo($fifos{hash}, oct(600)) or die "unable to create fifo for hashing dump";
            push @temp_files, $fifos{hash};
        }
        my @cpipes;
        if ($config->mbuffer) {
            push (@cpipes,
                  {
                      'stage' => 'mbuffer-checksum',
                      'exec'  => \@mb,
                  },
              );
        }
        push(@cpipes,
             { 'stage' => 'checksum', 'exec' => [ $config->checksum_binaries->{$config->checksum_algorithm} ] },
         );
        push(@pipelines,{
            input_file    => $fifos{hash},
            output_handle => $cfh,
            pipes         => \@cpipes,
        });
    }

    # here we need to tack the compression step for our rsyncable
    # compression, provided it isn't the same as the default if any
    # onto out pipe structure
    if ($config->rsyncable and $config->rsyncable_compression ne $compression) {
        unless($config->dry_run) {
            mkfifo($fifos{rsync}, oct(600)) or die "unable to create fifo for hashing dump";
            push @temp_files, $fifos{rsync};
        }
        my @rpipes;
        if ($config->mbuffer) {
            push (@rpipes,
                  {
                      'stage' => 'mbuffer-rsync',
                      'exec'  => \@mb,
                  },
              );
        }
        push(@rpipes,{
            'stage' => 'rsync-compress',
            'exec' => [
                split(/\s+/,$config->compressor_compress->{$config->rsyncable_compression})
            ],
        });
        push(@pipelines,{
            input_file  => $fifos{rsync},
            output_file => $rsyncable_path,
            pipes       => \@rpipes,
        });
    }
    if ($decomp_hash) {
        push @pipelines, $decomp_hash;
    }

    if($config->dry_run) {
        dry_run_pipeline(@pipelines);
        return(0,'dump');
    }
    my $exit_map = pipeline(@pipelines);

    foreach my $f (values(%fifos)) {
        unlink $f if -p $f;
    }

    $logparams{runtime} = time() - $start_time;
    $logparams{exit} = 0;
 PIPELINE:
    foreach my $line (@pipelines) {
    PIPE:
        foreach my $p (@{$$line{pipes}}) {
        STAGE:
            my $stage = $$p{stage};
            if (exists $$exit_map{$stage} and $$exit_map{$stage} != 0) {
                $logparams{exit} = $$exit_map{$stage};
                $logparams{failed_stage} = $stage;
                last PIPELINE;
            }
        }
    }
    if ( $logparams{exit} == 0 and $config->checksum ) {
        my $fh;
        unless(open($fh,'<',$cfp)) {
            $logparams{errno} = $ERRNO;
            dlog('error', 'checksum file open failure', \%logparams);
        }
        my $line;
    EOF:
        while ( ! eof($fh) ) {
            unless ( defined( $line = readline $fh ) ) {
                $logparams{errno} = $ERRNO;
                dlog('error', 'checksum file read error', \%logparams);
            }
            last EOF;
        }
        if($line and my ($hash) = $line =~ m{\A(.+?)\s+-}xms) {
            $logparams{$config->checksum_algorithm} = $hash;
            update_checksum_db($dbtype,
                               $database,
                               $start_time,
                               $config->checksum_algorithm,
                               $hash);
        }
        else {
            dlog('error', 'checksum parse failure', \%logparams);
        }
        close $fh;
    }
    unlink $cfp if (defined $cfp and -f $cfp);
    dlog( ($logparams{exit} == 0 ? 'info' : 'error'),
          ($logparams{exit} == 0 ? "succeeded" : "failed").
          ": dump of ${dbtype}:${database} exited with $logparams{exit}",
          \%logparams,
          { 'msgcode' => 'dump status' });
    return ($logparams{exit}, $logparams{failed_stage});
}

sub dry_run_pipeline {
    for my $line (@_) {
        for my $p (0..(scalar @{$$line{pipes}}-1)) {
            print ' | ' if $p != 0;
            print join(' ',@{$$line{pipes}[$p]{exec}});
            print ' < ' . $$line{input_file} if $p == 0 and $$line{input_file};
            print ' > ' . $$line{output_file} if $p == (scalar @{$$line{pipes}}-1) and $$line{output_file};
        }
        print "\n";
    }
}

sub build_postgresql_exec {
    my $bin = shift;
    my @com = ($config->postgresql_bindir ? File::Spec->catfile($config->postgresql_bindir,$bin) : $bin);

    if($bin eq 'psql') {
        # no special options here
    }
    else {
        if ( $config->postgresql_extra_option and scalar @{$config->postgresql_extra_option} > 0 ) {
            push(@com, @{$config->postgresql_extra_option});
        }
    }
    if ($config->postgresql_username) {
        push(@com, '--username='.$config->postgresql_username);
    }
    if ($config->postgresql_host) {
        push(@com, '--host='.$config->postgresql_host);
    }
    return @com;
}

sub update_checksum_db {
    my $databasetype = shift;
    my $databasename = shift;
    my $start_time = shift;
    my $checksum_algorithm = shift;
    my $hash = shift;

    my $lock = lock_db($CHECKSUM_DB,LOCK_EX);
    my %sums;
    unless(tie %sums, 'GDBM_File', $CHECKSUM_DB, O_CREAT|O_RDWR, oct(666)) {
        dlog('error', "unable to open database file ${CHECKSUM_DB}", {db_file => $CHECKSUM_DB});
        return;
    }
    $sums{make_checksum_key($databasetype,
                            $databasename,
                            strftime($timestamp_format,localtime(int($start_time))))
      } = "${checksum_algorithm}=${hash}";
    untie %sums;
    unlock_and_close($lock);
}

sub make_checksum_key {
    my ($t, $d, $s) = @_;
    return sprintf "%s*%s@%s", lc($t), $d, $s;
}

sub split_checksum_key {
    if( my ($t,$d,$s) = $_[0] =~ m{(.+) [*] (.+) [@] (.+)}xms ) {
        return unless str2time($s);
        return($t,$d,$s,str2time($s));
    }
    return;
}

sub checksum_dump {
    local %logparams = %logparams;
    my $lock = lock_db($CHECKSUM_DB,LOCK_SH);
    my %sums;
    tie %sums, 'GDBM_File', $CHECKSUM_DB, O_CREAT|O_RDWR, oct(666) or die "unable to tie ${CHECKSUM_DB}";
    while(my ($k,$v)=each(%sums)) {
        my ($t,$d,$s,$ss) = split_checksum_key($k);
        print sprintf "%s\t%s\t%s: %s\n", $t, $d, $s, $v =~ /[=]/ ? $v : "sha256=${v}";
    }
    untie %sums;
    unlock_and_close($lock);
}

# maybe this should remove missing entries from the database, but not
# for now, just delete unparseable entries!
sub checksum_fsck {
    local %logparams = %logparams;
    my $lock = lock_db($CHECKSUM_DB,LOCK_SH);
    my %sums;
    my @deletes;
    tie %sums, 'GDBM_File', $CHECKSUM_DB, O_CREAT|O_RDWR, oct(666) or die "unable to tie ${CHECKSUM_DB}";
    while(my ($k,$v)=each(%sums)) {
        my ($t,$d,$s,$ss);
        unless(($t,$d,$s,$ss) = split_checksum_key($k)) {
            warn "unable to parse garbage: $k, deleting";
            push(@deletes,$k);
            next;
        }
        print sprintf "%s\t%s\t%s: %s\n", $t, $d, $s, $v =~ /[=]/ ? $v : "sha256=${v}";
    }
    my $dumps = scan_existing();
    while(my ($k,$v)=each(%sums)) {
        my ($t,$d,$s,$ss);
        unless(($t,$d,$s,$ss) = split_checksum_key($k)) {
            warn "unsplittable key ${k}";
            next;
        }
        unless ( exists $$dumps{$t}{$d} and find_dump_by_timestamp(str2time($s),$$dumps{$t}{$d}) ) {
            dlog('warning',
                 sprintf("missing file for $t:$d at %s",
                         strftime($timestamp_format,
                                  localtime(str2time($s)))),
                 \%logparams,
                 {databasetype => $t, databasename => $d, start_time => $s});
            push(@deletes,$k);
        }
    }
    foreach my $k (@deletes) {
        delete $sums{$k};
    }
    untie %sums;
    unlock_and_close($lock);
}

# given dbtype, dbname, and timestamp:
# return algo and hash value if it exists
sub checksum_lookup {
    my $key = make_checksum_key($_[0],$_[1],strftime($timestamp_format,localtime(int($_[2]))));
    return unless $key;
    my $lock = lock_db($CHECKSUM_DB,LOCK_SH);
    my %sums;
    tie %sums, 'GDBM_File', $CHECKSUM_DB, O_CREAT|O_RDWR, oct(666) or die "unable to tie ${CHECKSUM_DB}";
    my ($a,$v) = (undef,undef);
    if ( exists $sums{$key} ) {
        unless( ($a,$v) = $sums{$key} =~ /^(.+)=(.+)/xms ) {
            $a = 'sha256';
            $v = $sums{$key};
        }
    }
    untie %sums;
    unlock_and_close($lock);
    return ($a,$v);
}

sub checksum_verify_all {
    local %logparams = %logparams;
    my $lock = lock_db($CHECKSUM_DB,LOCK_SH);
    my %_sums;
    tie %_sums, 'GDBM_File', $CHECKSUM_DB, O_CREAT|O_RDWR, oct(666) or die "unable to tie ${CHECKSUM_DB}";
    my %sums = %_sums; # does this work?
    untie %sums;
    unlock_and_close($lock);

    my $dumps = scan_existing();
    # files get deleted so there is no reason to alert on this
    # while(my ($k,$v)=each(%sums)) {
    #     my ($t,$d,$s,$ss) = split_checksum_key($k) or die "unsplittable key ${k}";
    #     unless ( exists $$dumps{$t}{$d}{str2time($s)} ) {
    #         dlog('warning',"missing file",\%logparams,{databasetype => $t, databasename => $d, start_time => $s});
    #     }
    # }
    while(my ($dbtype,$h) = each(%{$dumps})) {
        while(my ($db,$th) = each(%{$h})) {
            foreach my $dh (@{$th}) {
                #warn "${dbtype}\t${db}\t${ts}" if $DEBUG;
                my $checksum_key = make_checksum_key($dbtype,
                                                     $db,
                                                     strftime($timestamp_format,
                                                              localtime(int($$dh{timestamp}))));
                if( exists $sums{$checksum_key} ) {
                    # warn sprintf "found dump in existing db: %s %s at %s with checksum %s stored in %s",
                    #     $dbtype,
                    #     $db,
                    #     strftime($timestamp_format,localtime(int($ts))),
                    #     $sums{$checksum_key},
                    #     $$dh{path};
                    $logparams{databasetype} = lc $dbtype;
                    $logparams{databasename} = $db;
                    $logparams{start_time} = strftime($timestamp_format,
                                                      localtime(int($$dh{timestamp})));
                    $logparams{checksum} = $sums{$checksum_key};
                    checksum_verify_dump($dumps,$dh,$sums{$checksum_key});
                }
            }
        }
    }
}

sub find_dump_from_file {
    local %logparams = %logparams;
    my $path = shift;
    my $dumps = scan_existing();
 DBTYPE:
    while(my ($dbtype,$h) = each(%{$dumps})) {
        while(my ($db,$th) = each(%{$h})) {
            foreach my $dh (@{$th}) {
                if ( $$dh{path} eq $path ) {
                    return $dh;
                }
            }
        }
    }
    return; # failed to find
}

sub checksum_verify {
    local %logparams = %logparams;
    my $path = shift;

    my $dumps = scan_existing();
    my $dump = build_dump_hash_from_path($path);
    unless ($dump) {
        dlog('error',"file not parseable as dump",\%logparams,{path => $path});
        die "file not parseable as dump: ${path}";
    }
    if ($$dump{broken}) {
        dlog('error', "checksum broken", \%logparams);
        return;
    }
    my $lock = lock_db($CHECKSUM_DB,LOCK_SH);
    my %_sums;
    tie %_sums, 'GDBM_File', $CHECKSUM_DB, O_CREAT|O_RDWR, oct(666) or die "unable to tie ${CHECKSUM_DB}";
    my %sums = %_sums; # does this work?
    untie %sums;
    unlock_and_close($lock);

    my $checksum_key = make_checksum_key(lc($$dump{databasetype}),
                                         $$dump{databasename},
                                         strftime($timestamp_format,
                                                  localtime(int($$dump{timestamp}))));
    $logparams{databasetype} = lc $$dump{databasetype};
    $logparams{databasename} = $$dump{databasename};
    $logparams{start_time} = strftime($timestamp_format,
                                      localtime(int($$dump{timestamp})));
    if( exists $sums{$checksum_key} ) {
        $logparams{checksum} = $sums{$checksum_key};
        return checksum_verify_dump($dumps,$dump,$sums{$checksum_key});
    }
    else {
        dlog('warning',"checksum missing", \%logparams);
    }
    return;
}

sub restore {
    local %logparams = %logparams;
    my $path = shift;

    my $dumps = scan_existing();
    my $dump = build_dump_hash_from_path($path);
    unless ($dump) {
        dlog('error',"file not parseable as dump",\%logparams,{path => $path});
        die "file not parseable as dump: ${path}";
    }
    if($$dump{broken}) {
        dlog('error', "restore broken", \%logparams, {path => $path});
        die "unable to restore broken link: ${path}";
    }
    my ($input_file,$pipes) = reconstruct_pipeline($dumps,$dump);
    unless(defined $input_file) {
        dlog('error',
             "unable to construct a pipeline to restore this file! (probably missing parent of vcdiff file!)",
             \%logparams);
        return;
    }
    print STDERR Dumper $pipes if $DEBUG;
    return pipeline({
        input_file  => $input_file,
        output_file => '/dev/stdout',
        pipes       => $pipes,
    });
}

sub find_dump_by_timestamp {
    my $timestamp = shift;
    my $dumps = shift;
    foreach my $dump (@$dumps) {
        return $dump if int($$dump{timestamp}) == int($timestamp);
    }
    return;
}

sub reconstruct_pipeline {
    my $dumps = shift; # all the dumps so xdelta can find parents
    my $dump = shift;
    local %logparams = (%logparams,%$dump);
    -f $$dump{path} or die "file to be restored doesn't exist: $$dump{path}";
    my @mb = ( $config->mbuffer_binary, '-q' );

    my @pipes;
    my $input_file;

    if (my $compression = find_compressor($$dump{path})) {
        print STDERR Dumper $config->compressor_uncompress->{$compression} if $DEBUG;
        if ($compression eq 'xdelta') {
            my $parent = find_dump_by_timestamp( $$dump{full_timestamp},
                $$dumps{ $$dump{databasetype} }{ $$dump{databasename} } );
            unless( $parent ) {
                dlog('error',"unable to find parent of $$dump{path}",\%logparams);
                return;
            }
            $logparams{full_path} = $$parent{path};
            print STDERR Dumper $parent if $DEBUG;
            if ( my $secondary_compression = find_compressor($$parent{path}) ) {
                print STDERR Dumper $config->compressor_uncompress->{$secondary_compression} if $DEBUG;
                $input_file = $$parent{path};
                push(@pipes,
                     {
                         'stage' => 'uncompress-parent',
                         'exec'  => [split(/\s+/,$config->compressor_uncompress->{$secondary_compression})],
                     },
                 );
                push @pipes, { 'stage' => 'mbuffer-parent', exec => [@mb] } if $config->mbuffer;
                push(@pipes,
                     {
                         'stage' => 'uncompress',
                         'exec'  => [split(/\s+/,$config->compressor_uncompress->{$compression}),
                                     '-s',
                                     '/dev/stdin',
                                     $$dump{path}],
                     }
                 );
            }
            else { # uncompressed parent file
                # input_file is undef
                push(@pipes,{
                    'stage' => 'uncompress',
                    'exec'  => [split(/\s+/,$config->compressor_uncompress->{$compression}),
                                '-s',
                                $$parent{path},
                                $$dump{path}],
                });
            }
            push @pipes, { 'stage' => 'mbuffer-uncompress', exec => [@mb] } if $config->mbuffer;
        }
        else {
            # normal compression
            $input_file = $$dump{path};
            push @pipes, { 'stage' => 'mbuffer-pre', exec => [@mb] } if $config->mbuffer;
            push(@pipes,{
                'stage' => 'uncompress',
                'exec'  => [split(/\s+/,$config->compressor_uncompress->{$compression})],
            });
            push @pipes, { 'stage' => 'mbuffer-uncompress', exec => [@mb] } if $config->mbuffer;
        }
    }
    else { #uncompressed
        # use "cat" since we are too lazy to use File::Cat and File::Tee?
        $input_file = $$dump{path};
        push(@pipes,
             {
                 'stage' => 'uncompress',
                 'exec' => $config->mbuffer ? \@mb : [split(/\s+/,$config->compressor_uncompress->{'none'})],
             },
         );
    }
    print Dumper \@pipes if $DEBUG;
    return($input_file,[@pipes]);
}

sub checksum_verify_dump {
    local %logparams = %logparams;
    my $dumps = shift; # all the dumps so xdelta can find parents
    my $dump = shift;
    print STDERR Dumper $dump if $DEBUG;
    my $_checksum = shift;
    my ($algo,$checksum);
    unless( ($algo,$checksum) = $_checksum =~ m{(.+) [=] (.+)}xms ) {
        $algo = 'sha256';
        $checksum = $_checksum;
        dlog('debug', "unable to parse algorithm from key value: ${_checksum}",
             \%logparams);
    }
    unless(exists $config->checksum_binaries->{$algo}) {
        dlog('error', "specified algorithm isn't defined in the config: ${algo}",
             \%logparams);
        return;
    }
    -f $$dump{path} or die "file to be verified doesn't exist: $$dump{path}";
    my ($th,$tf) = tempfile();
    push @temp_files, $tf;
    my ($input_file,$pipes) = reconstruct_pipeline($dumps,$dump);
    unless( defined $input_file ) {
        # failed to find!
        dlog('error',
             "unable to verify checksum for $$dump{path}, parent not found!",
             \%logparams);
        return;
    }
    push @{$pipes}, { 'stage' => 'checksum', 'exec' => [ $config->checksum_binaries->{$algo} ] };
    print Dumper $pipes if $DEBUG;
    my $e = pipeline({
        input_file  => $input_file,
        output_file => $tf,
        pipes => $pipes,
    });

    my $actual_checksum = checksum_from_file($tf);
    close $th;unlink $tf;
    pop @temp_files; # we tried, no need to clutter up temp files
    if($actual_checksum and $checksum eq $actual_checksum) {
        dlog('info',"${algo} checksum match $$dump{path} ${checksum}",
             \%logparams,{path => $$dump{path}, msgcode => 'checksum mismatch'});
        return $checksum;
    }
    dlog('error',"${algo} checksum mismatch $$dump{path} = ${actual_checksum} != ${checksum}",\%logparams,{path => $$dump{path}, msgcode => 'checksum mismatch'});
    return;
}

sub checksum_from_file {
    local %logparams = %logparams;
    my $file = shift;
    my $fh;
    unless(open($fh,'<',$file)) {
        $logparams{errno} = $ERRNO;
        dlog('error', 'checksum file open failure', \%logparams);
    }
    my $line;
 EOF:
    while ( ! eof($fh) ) {
        unless ( defined( $line = readline $fh ) ) {
            $logparams{errno} = $ERRNO;
            dlog('error', 'checksum file read error', \%logparams);
            close $fh;
            return;
        }
        last EOF;
    }
    if($line and my ($hash) = $line =~ m{ \A (.+?) \s+ [-] }xms) {
        close $fh;
        return $hash;
    }
    else {
        dlog('error', 'checksum parse failure', \%logparams);
    }
    close $fh;
    return;
}

sub sanity_check_config {
    my $msg;
    foreach my $compy ($config->compression,
                       $config->secondary_compression,
                       $config->rsyncable_compression) {
        next unless $compy;
        unless( defined $config->compressor_compress->{$compy} and
                defined $config->compressor_uncompress->{$compy} and
                ($compy eq 'none' ? 1 : defined $config->compressor_suffix->{$compy}) ) {
            $msg = sprintf "requested compression (%s/%s) is not defined in the config!", $config->compression,$config->secondary_compression;
            dlog('critical',
                 $msg,
                 {
                     'compression' => $config->compression,
                     'secondary_compression' => $config->secondary_compression,
                 });
            die $msg;
        }
    }
    if($config->rsyncable_compression eq 'xdelta') {
        $msg='xdelta is not valid for rsyncable_compression option';
        dlog('error',$msg,{});
        die $msg;
    }
  unless ($config->mysql or $config->postgresql or $config->mongodb) {
    $msg='neither mysql nor postgresql not mongodb was specified, nothing to dump!';
    dlog('error',$msg,{});
    die $msg;
  }
}

# this is invoked after a successful backup
# turn symlinks in per-period directories into hardlinks to the actual
# file, because the prior backup/dump to which it is currently
# symlinked is about to be deleted or overwritten
# IF the target is a .TEMP file it's because localcow was enabled and
# therefore we copied the backup/dump file in current in order to
# hardlink to it because we are going to overwrite it using rsync
# shortly

sub link_symlinks {
    my %logparams;
    my $dbtype        = shift;
    my $dbname        = shift;
    return unless defined $dump_db{$dbtype}{$dbname};
    foreach my $dump (@{$dump_db{$dbtype}{$dbname}}) {
        next unless -l $$dump{path}; # skip the non-links
        print Dumper $dump if $DEBUG;
        my $target = readlink($$dump{path});
        my $oldwd = POSIX::getcwd();
        chdir dirname($$dump{path}) or die "unable to chdir to ${$$dump{path}}";
        if (-f join('.',$target,'TEMP')) {
            $target =  join('.',$target,'TEMP');
        }
        $logparams{dump_path}     = $$dump{path};
        $logparams{target}        = $target;

        # remove this link whether the target exists or not, if
        # the target of a link is missing it's useless anyway:
        if ( unlink($$dump{path}) ) {
            dlog( 'debug', "unlink symlink $$dump{path}", {msgcode => 'unlink symlink'}, \%logparams );
        }
        else {
            dlog( 'error', "unlink symlink $$dump{path}", \%logparams, { errno => $ERRNO, msgcode => 'unlink symlink' } );
        }
        # create a hard link to a path that's about to disappear:
        if ( link( $target, $$dump{path} ) ) {
            dlog( 'debug', "link former symlink $$dump{path} -> ${target}", {msgcode => 'link symlink'}, \%logparams );
        }
        else {
            dlog(
                'error', "failed to link former symlink $$dump{path} -> ${target}",
                \%logparams, { msgcode => 'link symlink', errno => $ERRNO }
            );
        }
        chdir($oldwd) or die "unable to chdir to ${oldwd}";
    }
}

sub rsync {
    my ( $src, $dst ) = @_;
    my %logparams = (
        src => $src,
        dst => $dst,
        rsync_binary => $config->rsync_binary,
        rsync_options => $config->rsync_options,
    );
    # because we use in place by default, we should consider the destination sus:
    dlog('debug',
         "rsync file copy from ${src} to ${dst}",
         \%logparams);
    my $result = system_runner(
        sprintf( '%s %s %s %s',
                 $config->rsync_binary, $config->rsync_options, $src, $dst ),
    );
    $logparams{result} = $result == 1 ? 'succeeded' : 'failed';
    dlog($result == 1 ? 'info' : 'debug',
         "${logparams{result}}: rsync file copy from ${src} to ${dst}",
         \%logparams);
    return $result;
}

# copied from the old version of List::Util:
sub string_any {
    my $s = shift;
    foreach (@_) {
        return 1 if $s eq $_;
    }
    return 0;
}

# update the state database
sub update_status_db {
    my ($dbtype, $dbname, $value) = @_;

    if ( defined $$value{start_time} ) {
        $$value{start_time} = strftime($timestamp_format, localtime($$value{start_time}));
    }
    my $db_lock_handle = lock_db($DB_FILE,LOCK_EX);

    my %status;
    unless(tie %status, 'GDBM_File', $DB_FILE, O_CREAT|O_RDWR, oct(666)) {
        dlog('error', "unable to open database file ${DB_FILE}", {db_file => $DB_FILE});
        return;
    }
    my $json = JSON->new();
    $status{join('*', $dbtype, $dbname)} = $json->encode($value);

    untie %status;
    unlock_and_close($db_lock_handle);
}

sub status_delete {
    my $h = lock_db($DB_FILE,LOCK_EX);

    my %status;
    unless(tie %status, 'GDBM_File', $DB_FILE, O_RDWR, oct(666)) {
        dlog('error', "unable to open database file ${DB_FILE} for deletions", {db_file => $DB_FILE});
        return;
    }

    for my $k (@_) {
        delete $status{$k} if exists $status{$k};
    }

    untie %status;
    unlock_and_close($h);
}

sub status_json {
    my $h = lock_db($DB_FILE,LOCK_SH);

    my %status;
    unless(tie %status, 'GDBM_File', $DB_FILE, O_RDONLY, oct(666)) {
        dlog('error', "unable to open database file ${DB_FILE} for reading", {db_file => $DB_FILE});
        return;
    }
    my %data;
    my $json = JSON->new->pretty();
 STATUS:
    while(my ($k,$v)=each(%status)) {
        eval {
            $data{$k} = $json->decode($v);
        }
        or do {
            eval {
                $data{$k} = thaw($v);
            }
            or do {
                dlog('error', "unable to decode string in status database for ${k}", {db_file => $DB_FILE});
                next STATUS;
            };
        };
    }
    untie %status;
    unlock_and_close($h);
    print $json->encode(\%data);
}

sub build_dump_hash_from_path {
    local %logparams = %logparams;
    my $file = shift;
    $logparams{'file'} = $file;

    state $d_q = '('.join('|',@dbtypes).')';
    state $d_p = '('.join('|',@periods).')';

    my $full_timestamp;
    my $timestamp;
    my ($type, $db, $period, $o, $i, $year, $month, $day, $hour, $minute);

    my %dump = ( path => $file, full => $file =~ m{[.] vcdiff}xms ? 0 : 1, broken => 0 );
    if ( -l $file and not -f $file ) {
        # broken link
        dlog( 'error', "broken link ${file}", {msgcode => 'broken link'}, \%logparams );
        $dump{broken} = 1;
    }
    if ( ($db, $period, $year,$month,$day,$hour,$minute) = basename($file) =~ m{\A(.+) [.-] ${d_p} [.-] sql [.-] (\d\d\d\d) [.-] (\d\d) [.-] (\d\d) [.-] (\d\d) [.-] (\d\d)}xms ) {
        $timestamp = mktime(0,
                            $minute,
                            $hour,
                            $day,
                            ($month - 1),
                            ($year - 1900)
                        );
        $dump{timestamp} = $timestamp;
        $dump{period} = $period;
        $dump{full} = 1;
        $dump{databasename} = $db;
        $dump{databasetype} = 'mysql';
    }
    elsif ( ($db, $year, $month, $day, $hour, $minute )
            = basename($file)
            =~ m{\Apostgres [.-] (.+) [.-] sql [.-] (\d\d\d\d) [.-] (\d\d) [.-] (\d\d) [.-] (\d\d) [.-] (\d\d)}xms ) {
        $type = 'postgresql';
        $timestamp = mktime(0,
                            $minute,
                            $hour,
                            $day,
                            ($month - 1),
                            ($year - 1900)
                        );
        $dump{timestamp} = $timestamp;
        $dump{period} = basename(dirname($file));
        $dump{databasetype} = lc $type;
        $dump{databasename} = $db;
    }
    elsif ( ($db, $year, $month, $day ) = basename($file)
            =~ m{\Apostgres [.-] (.+) [.-] sql [.-] (\d\d\d\d) [.-] (\d\d) [.-] (\d\d)}xms ) {
        $type = 'postgresql';
        $timestamp = mktime(0,
                            0,
                            0,
                            $day,
                            ($month - 1),
                            ($year - 1900)
                        );
        $dump{timestamp} = $timestamp;
        $dump{period} = basename(dirname($file));
        $dump{databasetype} = lc $type;
        $dump{databasename} = $db;
    }
    elsif ( ($type, $db, $o, $i) = basename($file) =~ m{\A${d_q} [-] (.+?) [-] ($iso8601_regex) [-_] ($iso8601_regex) [.] sql [.] vcdiff}xms ) {
        # files like: daily/mysql-enchantress-2022-10-04T08:50:45.sql.zst
        # should probably inlude more variations of the timestamp?
        $full_timestamp = str2time($o) or die "unable to parse ${o} using str2time";
        $timestamp = str2time($i) or die "unable to parse ${i} using str2time";
        $dump{timestamp} = $timestamp;
        $dump{full_timestamp} = $full_timestamp;
        $dump{period} = basename(dirname($file));
        $dump{databasetype} = lc $type;
        $dump{databasename} = $db;
    }
    elsif ( ($type, $db, $i) = basename($file) =~ m{\A${d_q} [.-] (.+?) [.-] ($iso8601_regex) [.] sql}xms ) {
        # files like: daily/mysql-enchantress-2022-10-04T08:50:45.sql.zst
        # should probably inlude more variations of the timestamp?
        $timestamp = str2time($i) or die "unable to parse ${i} using str2time";
        $dump{timestamp} = $timestamp;
        $dump{period} = basename(dirname($file));
        $dump{databasetype} = lc $type;
        $dump{databasename} = $db;
    }
    else {
        dlog( 'error', "unable to parse time stamp from file", \%logparams );
        return;
    }
    return \%dump;
}


=head1 NAME

delta-dumper - wrapper for mysqldump, pg_dump, and xdelta

=head1 SYNOPSIS

B<delta-dumper> {B<-h|--help>} {B<--mysql|--no-mysql>} {B<--postgresql|--no-postgresql>}

=head1 DESCRIPTION

Parse a config file and command line, invoke various dump programs.

=head1 OPTIONS

=head2 MAJOR MODES

delta-dumper's default mode is to dump databases and write them to the
configured location.  What follows are a list of options that perform
the documented action, then exit I<without> performing backups.

=over 4

=item B<--restore>=I<dump>

delta-dumper will effectively cat the output of the dump specified to
the terminal, uncompressing or un-xdelta encoding the file as needed.
The full path to the dump file must be specified, you cannot use the
files in the "current" directory, only those found in daily, weekly,
etc.

Example:
delta-dumper --restore /opt/csg-db-backup/daily/mysql-enchantressdev-2023-09-08T05:15:01-0500_2023-09-10T05:15:01-0500.sql.vcdiff

=item B<--status-json>

Print the status database as JSON.  Used by the check_mk plugin.
Provides a moderately human-readable status of the last database
dumps.

=item B<--status-delete>=I<key>

Use the same keys as printed in the --status-json output.

Example:
delta-dumper --status-delete 'mysql*mysql'

=item B<--checksum-verify>=I<dump>

delta-dumper will uncompress the dump file as needed and calculate the
checksum, then compare it with the checksum stored in the database.
The full path to the dump file must be specified, you cannot use the
files in the "current" directory, only those found in daily, weekly,
etc.

Example:
delta-dumper --checksum-verify /opt/csg-db-backup/daily/mysql-enchantressdev-2023-09-08T05:15:01-0500_2023-09-10T05:15:01-0500.sql.vcdiff

=item B<--checksum-verify-all>

delta-dumper will attempt to verify the checksum of every dump in it's
internal database of dump checksums, if those those dumps still exist
at the configuration backup location.

=item B<--checksum-dump>

Dump the contents of the databse containing sha266 checksums of
database dumps.  Mostly useful for verification purposes and
debugging.

=back

=head2 CONFIGURATION OPTIONS

=over 4

=item B<--mysql|--no-mysql> C<mysql>

Default: false

Perform (or don't) mysql database dumps.

=item B<--postgresql|--no-postgresql> C<postgresql>

Default: false

Perform (or don't) postgresql database dumps.

=item B<--mongodb|--no-mongodb> C<mongodb>

Default: false

Perform (or don't) mongodb database dumps.

=item B<--dry_run|--dry-run|--no-dry_run|--no-dry-run> C<dry_run>

When set, print but don't execute commands or system calls that would
take any real action.

=item B<--backup_location>=I<dir> C<backup_location>

This is where the final backups will end up, in sub-directories named
weekly, current, etc.

=item B<--old_backup_location>=I<dir> C<old_backup_location>

Look here for backups created previously by delta-dumper,
csg-db-backup.pl, and pgsql scripts in these directories so
delta-dumper can clean up from old programs or old dumps stored in old
locations.

=item B<--tmpdir>=I<dir> C<tmpdir>

Directory used to write the output of dump commands.  The default is
to use the 'current' directory in the C<backup_location> directory.

=item B<--rsyncable|--no-rsyncable> C<rsyncable> (Boolean)

When rsyncable is set delta-dumper tries to overwrite the same
unchanging file name in /current/ of the backup_location or
C<rsyncable_location> if set.

delta-dumper uses rsync --inplace --no-whole-file to update only
changed parts of the backup.  This is best paired with compression
methods that have an --rsyncable option but still works with other
compression methods.  The goal of rsyncable mode is to make minimal
changes to the file on COW-style filesystem (zfs, btrfs, and many
more) to avoid bloating snapshots.

=item B<--rsyncable_location>=I<dir> C<rsyncable_location>

This is where the rsyncable fixed filename backups are placed if
C<rsyncable> is set.  When not set, use /current/ in backup_location.

=item B<--rsyncable_compression>=I<method> C<rsyncable_compression>

Use this compression method when C<rsyncable> is set. Cannot be
xdelta, because that makes no sense for C<rsyncable>'s use-case.

=item B<--postrun>=I<command> C<postrun>

Run this command after all (not each) dumps.

=item B<--prerun>=I<command> C<prerun>

Run this command before all (not each) dumps.

=item B<--mbuffer|--no-mbuffer> C<mbuffer>

Insert mbuffer at various stages in the pipeline.

=item B<--mbuffer_binary|--mbuffer-binary>=I<binary> C<mbuffer_binary>

Execute this when mbuffer is enabled.

Default: mbuffer

=item B<--mbuffer_opts|--mbuffer-opts>=I<opts> C<mbuffer_opts>

Append these options to mbuffer invocations. (ie: -m 1G)

=item B<--rate_limit|--rate-limit>=I<rate> C<mbuffer_opts>

Pass this option to pv -L, that is set the rate limitting factor for
pv for testing.

=item B<--checksum|--no-checksum> C<checksum>

Write the checksum of the dump to the log file and database.

=item B<--checksum_binary|--checksum-binary> C<checksum_binary>

Pipe dump data to this binary.  Assumed to creat output like, md5sum,
sha256sum, etc.

Default: sha256sum


=back

=head2 LOGGING OPTIONS

=over 4

=item B<--file_logging|--no-file_logging> C<file_logging>

Default: true

Enable or disable logging to configured log location.  The minimum
severity (level) logged to the file is controlled by the log_level
parameter.

=item B<--sys_logging|--syslog|--no-sys_logging|--no-syslog> C<sys_logging>

Default: true

Enable or disable logging syslog.  The minimum severity (level) logged
to the file is controlled by the log_level parameter and the facility
used is configured with the paramater of the same name.

=item B<--terminal_logging|-t|--no-terminal_logging|--no-t> C<terminal_logging>

Default: true if stdout is a terminal, false otherwise.

Enable or disable logging to stderr.  The minimum severity (level)
logged to stderr is controlled by the log_level parameter.

=item B<--log_level|--level|--log-level|--loglevel> C<log_level> (String)

Default: info

Log messags of this severity and above will be written to the various
logging outputs, if enabled.  These correspond to the long names of
the syslog severities:

C<^(debug|info|notice|warning|error|critical|alert|emergency)$>

=item B<--facility> C<facility> (String)

Default: user

Use this facility when logging to syslog which must match:

C<^(auth|authpriv|cron|daemon|kern|local[0-7]|mail|news|syslog|user|uucp)$>

=back

=head2 COMPRESSION OPTIONS

=over 4

=item B<--compression> C<compresion> (String)

Default: gzip

Use pre-configured compression commands, gzip, bzip2, xz, and zstd.

Also, C<none> is accepted along with C<custom> which requires setting
the compress, uncompress, and suffix configuration items.

=item C<compressor_compress>
=item C<compressor_uncompress>
=item C<compressor_suffix>

[compressor]
compress gzip = gzip -c --rsyncable
uncompress gzip = gzcat
extension gzip = gz

=back

=head2 MYSQL OPTIONS

=over 4

=item B<--mysql_bindir>=I<dir> C<mysql_bindir>

If specified, use this directory to create the path to the mysql and
mysqldump executables.  Otherwise, assume they are in the path.

=item B<--mysql_defaults_file|--mysql_defaults-file|--defaults-file>=I<file>

When invoking mysql and mysqldump, call them with --defaults-file and
the value of this parameter.

=item B<--mysql_single_transaction|--no-mysql_single_transaction|--single-transaction|--no-single-transaction> (Boolean)

Default: true

Pass --single-transaction to mysqldump, or not.

=item B<--mysql_ignore_table>=I<table>

Default: mysql.events

Can be specified multiple times, each of which will be passed to the
--ignore-table option of mysqldump.

=item B<--mysql_user>=I<user>

Passed to -u on mysql and mysqldump commands.

=item B<--mysql_password>=I<password>

Passed to -p on mysql and mysqldump commands.

=item B<--mysql_hostname>=I<hostname>

Passed to -h on mysql and mysqldump commands.

=item B<--mysql_extra_option>=I<option> C<mysql_extra_option>

Can be specified multiple times, options passed to mysqldump, if set.

=item B<--mysql_verify|--no-mysql_verify> C<mysql_verify>

Checks for a final line containing the string "Dump Completed"

This is a flawed way to verify the database, but in general if
mysqldump doens't output this line you can assume something has gone
wrong.  However, the presence of this line in the output does not
guarantee that the dump was successful, only that it was "completed."

=item B<--mysql_skip_database>=I<db> C<mysql_skip_database>

List of databases to dump, can be specified multiple timees on the
command line and in the config file.

=back

=head2 POSTGRESQL OPTIONS

=over 4

=item B<--postgresql_bindir>=I<dir> C<postgresql_bindir>

If specified, use this directory to create the path to the postgresql
dump executables.  Otherwise, assume they are in the path.

=item B<--postgresql_skip_database>=I<db> C<postgresql_skip_database>

List of databases to dump, can be specified multiple timees on the
command line and in the config file.

=item B<--postgresql_extra_option>=I<option> C<postgresql_extra_option>

Default: --clean --if-exists

Can be specified multiple times, options passed to dump or dump all.

=item B<--postgresql_username>=I<username> C<postgresql_username>

Default: postgres

Passed to the --username= option of the dump programs.

=item B<--postgresql_host>=I<hostname> C<postgresql_host>

Passed to the --host= option of the dump programs.  If unset, don't
pass --host at all.

=back

=head2 MONGODB OPTIONS

=over 4

=item B<--mongodb_bindir>=I<dir> C<mongodb_bindir>

If specified, use this directory to create the path to the mongodb
dump executable.  Otherwise, assume they are in the path.

=item B<--mongodb_username>=I<username> C<mongodb_username>

Passed to the --username= option of the dump programs.

=item B<--mongodb_password>=I<password>

Passed to --password on mongodump command.

=item B<--mongodb_port>=I<port>

Passed to --port on mongodump command.

=item B<--mongodb_hostname>=I<hostname>

Passed to --hostname on mongodump command.

=item B<--mongodb_ssl|--no-mongodb_ssl>

Default: true

If true, pass --ssl to mongodump.

=back

=head2 RETENTION OPTIONS

=over 4

=item B<--daily>=I<count>

Keep <count> days worth of daily dumps.

=item B<--weekly>=I<count>

Keep <count> weeks worth of weekly dumps.

=item B<--monthly>=I<count>

Keep <count> months worth of monthly dumps.

=item B<--week_start>=I<day>

Default: Sun

When the day matches this string (Sun, Mon, etc.) then a weekly backup
will be created in addition to the daily.

=item B<--month_start>=I<day>

Default: 1

When the month day matches this a monthly backup will be created in
addition to the daily.

=item B<--maxage>=I<agespec> C<maxage>

When using xdelta compression, if the newest full backup is older than
this age specification create a new full backup instead of using
xdelta.

The specification is an integer followed by one of h, d, w, m, or y.

These correspond with hours, days, weeks, months, and years. Months
are considered to be 30 days long and years are 365 days.

So, if you want a new full dump every 2 weeks:

maxage = 2w

=back

=head2 RSYNC OPTIONS

Rsync is used when rename won't work, which is when the temporary
database file is across a filesystem boundary.

=over 4

=item B<--rsync_binary>

DEFAULT: rsync

Explicit path to rsync binary, if needed.

=item B<--rsync_options>=I<option>

DEFAULT: -a --inplace --no-whole-file

Options to pass to rsync when copying files.

=back

=head1 RETURN VALUE

=head1 ERRORS

=head1 DIAGNOSTICS

=head1 EXAMPLES

=head1 ENVIRONMENT

=head1 FILES

=head2 F</etc/delta-dumper/config>

Default config file when run as root.

=head2 F<$HOME/.config/delta-dumper/config>

Default config file when run as non-root.

=head1 CAVEATS

=head1 BUGS

=head1 RESTRICTIONS

=head1 NOTES

=head1 AUTHOR

Aran Cox <arancox@gmail.com>

=head1 HISTORY

=head1 SEE ALSO

=over 4

gzip(1), bzip2(1), zstd(1), xdelta(1)

=back
